# Realtime Financial Market Data Visualization and Analysis


# Introduction

In this project, I developed a financial data processing and visualization platform using ***Apache Kafka***, ***Apache Cassandra***, and ***Bokeh***. I used Kafka for realtime stock price and market news streaming, Cassandra for historical and realtime stock data warehousing, and Bokeh for visualization on web browsers. I also wrote a web crawler to scrape companys' financial statements and basic information from Yahoo Finance, and played with various economy data APIs. 




**Please check the platform's website and play with each plot** :  [magiconch.me](http://magiconch.me/)


# Architecture

There are currently 3 tabs in the webpage:

- ***Stock: Streaming & Fundamental*** 
  - Visualize a single stock's candlestick plot, basic company & financial information;
  - Plot realtime S&P500 price during trading hours (fake date during non-trading hours)
- ***Stock: Comparison***
  - Plot 2 user-selected stocks' price, and print their statstical summay and correlation
  - Visualize the 5,10,30-day moving average of adjusted close price
- ***Economy***
  - Plot a geomap to visualize various economy data by state
  - Plot 4 economy indicators nationwide for comparison
  - Present the most recent market news 

&nbsp;


Here is the architecture of the platform, and I will go through each part of it.

<img src="https://github.com/nancyyanyu/kafka_stock/blob/master/images/kafka_stock.png" width="800" />

## 1.Stock: Streaming & Fundamental

### 1.1 Data Source

- [Alpha Vantage](https://www.alphavantage.co/) : provide free APIs for realtime and historical data on stocks
  - Problems: rate limiting of 5 calls per minute, 500 calls per day  for free account 
- [Yahoo Finance](https://finance.yahoo.com/): write a web crawler to get company's summary profile and fundamental information such as financial statements



### 1.2 ETL

#### 1. Historical data

In the first tab, both historical data and streaming data are presented. Before each trading day, I got every stocks' historical data from ***Alpha Vantage*** to update the adjusted close and get the newest daily price and volume. 

The raw data is in string type, so transformation of the data type needs to be performed. I also standardized 'time' column to "%Y-%m-%d %H:%M:%S" because that's the format Cassandra could recognize.

Then I created table for each stock and stored the historical data to ***Apache Cassandra***, waiting to be queried by visualization module. Cassandra has its own query language ***Cassandra Query Language (CQL)*** which is quite similar to SQL.

```CQL
CREATE TABLE IF NOT EXISTS SYMBOL_historical ( 
       TIME timestamp,           
       SYMBOL text,              
       OPEN float,               
       HIGH float,               
       LOW float,                
       CLOSE float,              
       ADJUSTED_CLOSE float,     
       VOLUME float,             
       dividend_amount float,    
       split_coefficient float,  
       PRIMARY KEY (SYMBOL,TIME));
```



#### 2. Realtime data

I used Apache Kafka to handle streaming data, and Cassandra to store the realtime data, the stream plot would take both the stored data as well as the streaming data for visualization. The *topic* for this streaming task is *'stock_streaming1'*. 

This *topic* has two producers, which recevied fake data generated by random number and realtime S&P500 price and volume from ***Alpha Vantage***. It also has two consumers, which send data to Cassandra database and to a local json file.

<img src="https://github.com/nancyyanyu/kafka_stock/blob/master/images/stream1.png" width="600" />



**NOTE:**

Since ***Alpha Vantage*** could only be called 5 times per minute, so I could either get one minute frequency data by calling *TIME_SERIES_INTRADAY* function provided by  ***Alpha Vantage*** every minute , and , or to get data faster, call *GLOBAL_QUOTE* function every around 12 seconds to get the latest price and volume information, as well as avoid rate limiting error.

One other thing to note is **timezone**. I am in Pacific Time. To align with Eastern Time trading hours, I need to record the time I called *GLOBAL_QUOTE*, adjust to Eastern Time, and store accordingly to the database.

Interestingly, Cassandra stores datetime type data in UTC timezone. When querying data from Cassandra, we need to double check whether the time changed or not. Otherwise, the stored tick price would not seamlessly plot with streaming data.

